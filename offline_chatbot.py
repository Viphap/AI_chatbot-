# -*- coding: utf-8 -*-
"""Offline_Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17BVpZTqy-ypti-9A4Fcnexu6brAlFvVD
"""

# No colab imports
import os
import re
import json
import requests
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime, timedelta, timezone
from dotenv import load_dotenv
import difflib

# ================================================================
# âš™ï¸ CONFIG / AUTH
# ================================================================
load_dotenv()

TB_USER = os.getenv("TB_USER", "USER")
TB_PASS = os.getenv("TB_PASS", "PASS")
BASE_URL = os.getenv("BASE_URL") or "https://newsense.viphap.com/api"

# ================================================================
# ğŸ§  STATE GLOBALS
# ================================================================
kg_df = pd.DataFrame()
_lookup_built = False

# ================================================================
# ğŸ—ºï¸ BUILD LOOKUP TABLES Tá»ª KNOWLEDGE GRAPH (thay tháº¿ Gemini)
# ================================================================
# Tá»± Ä‘á»™ng Ä‘á»c táº¥t cáº£ vá»‹ trÃ­ & loáº¡i dá»¯ liá»‡u tá»« KG â€” khÃ´ng hardcode

# ================================================================
# ğŸ—ºï¸ NÃ‚NG Cáº¤P MAPPING & EXTRACTION
# ================================================================

def build_lookup_tables(kg_df: pd.DataFrame):
    if kg_df.empty:
        return {}, {}, {}, [], []

    kg_records = kg_df.to_dict(orient="records")

    # 1. Location Map (Giá»¯ nguyÃªn logic cÅ© nhÆ°ng bá»• sung clean key)
    location_col = next((c for c in kg_df.columns if "vá»‹ trÃ­" in c.lower() or "location" in c.lower()), None)
    location_map = {}
    if location_col:
        for val in kg_df[location_col].dropna().unique():
            val_str = str(val).strip()
            key = val_str.lower()
            location_map[key] = val_str
            # TÃ¡ch tá»« Ä‘á»ƒ báº¯t keyword táº¯t (vÃ­ dá»¥: "Ä‘á»‘t sk5" -> báº¯t "sk5")
            words = key.split()
            if len(words) >= 2:
                location_map[words[-1]] = val_str # Láº¥y tá»« cuá»‘i (thÆ°á»ng lÃ  mÃ£ Ä‘á»‘t)

    # 2. Semantic Type Map (Cáº¬P NHáº¬T Máº NH Máº¼ THEO DATA Cá»¦A Báº N)
    # Mapping tá»« ngÃ´n ngá»¯ tá»± nhiÃªn -> tá»« khÃ³a ká»¹ thuáº­t cÃ³ trong KG
    SEMANTIC_TYPE_MAP = {
        # Nhiá»‡t Ä‘á»™
        "nhiá»‡t Ä‘á»™": ["nhiá»‡t Ä‘á»™", "temp", "pt100", "nhiá»‡t", "t"],
        "nhiá»‡t":    ["nhiá»‡t Ä‘á»™", "temp", "pt100", "t"],
        "pt100":    ["pt100", "nhiá»‡t Ä‘á»™"],
        
        # á»¨ng suáº¥t / Strain Gauge
        "á»©ng suáº¥t": ["á»©ng suáº¥t", "stress", "strain", "sg", "contrainte", "biáº¿n dáº¡ng"],
        "strain":   ["strain", "sg", "contrainte"],
        "sg":       ["sg", "strain", "á»©ng suáº¥t", "contrainte"],
        "biáº¿n dáº¡ng":["biáº¿n dáº¡ng", "deformation", "strain", "sg"],
        
        # Dao Ä‘á»™ng / Gia tá»‘c
        "dao Ä‘á»™ng": ["dao Ä‘á»™ng", "vibration", "acc", "gia tá»‘c"],
        "rung":     ["rung", "vibration", "acc"],
        "gia tá»‘c":  ["gia tá»‘c", "acc", "acceleration"],
        
        # Chuyá»ƒn vá»‹ / GNSS
        "chuyá»ƒn vá»‹": ["chuyá»ƒn vá»‹", "displacement", "gnss", "gps"],
        "lÃºn":       ["lÃºn", "settlement"],
        "gnss":      ["gnss", "gps", "chuyá»ƒn vá»‹"],
        
        # MÃ´i trÆ°á»ng
        "giÃ³":       ["giÃ³", "wind"],
        "mÆ°a":       ["mÆ°a", "rain", "lÆ°á»£ng mÆ°a"],
        "Ä‘á»™ áº©m":     ["Ä‘á»™ áº©m", "humidity"],
    }

    # 3. Exact Var Map (Map chÃ­nh xÃ¡c tÃªn biáº¿n náº¿u cÃ³)
    var_col = next((c for c in kg_df.columns if "tÃªn biáº¿n" in c.lower() or "key" in c.lower()), None)
    type_map = {}
    if var_col:
        for var_val in kg_df[var_col].dropna().unique():
            key = str(var_val).lower().strip()
            type_map[key] = var_val

    # 4. Dynamic Device Types
    type_col = next((c for c in kg_df.columns if "loáº¡i" in c.lower()), None)
    dynamic_device_types = []
    if type_col:
        dynamic_device_types = [str(x).lower().strip() for x in kg_df[type_col].dropna().unique()]

    return location_map, SEMANTIC_TYPE_MAP, type_map, dynamic_device_types, kg_records

def extract_data_types(text: str) -> list[str]:
    """
    NÃ¢ng cáº¥p: Loáº¡i bá» stopwords vÃ  tÃ¬m keyword ká»¹ thuáº­t chÃ­nh xÃ¡c hÆ¡n.
    """
    text_lower = text.lower()
    
    # Danh sÃ¡ch tá»« vÃ´ nghÄ©a cáº§n loáº¡i bá» Ä‘á»ƒ trÃ¡nh báº¯t nháº§m
    stopwords = ["cá»§a", "táº¡i", "á»Ÿ", "cÃ¡c", "nhá»¯ng", "cÃ¡i", "xem", "cho", "biáº¿t", "vá»", "trong", "ngÃ y", "thÃ¡ng", "nÄƒm"]
    for w in stopwords:
        text_lower = text_lower.replace(f" {w} ", " ")

    found_patterns = set()

    # 1. QuÃ©t Semantic Map (Æ¯u tiÃªn cao)
    # VÃ­ dá»¥: user gÃµ "á»©ng suáº¥t" -> map ra ["sg", "contrainte", "stress"...]
    for keyword, related_terms in semantic_type_map.items():
        if keyword in text_lower:
            found_patterns.update(related_terms)

    # 2. QuÃ©t Dynamic Types (Loáº¡i thiáº¿t bá»‹ cÃ³ sáºµn trong KG)
    for dt in dynamic_device_types:
        if dt in text_lower:
            found_patterns.add(dt)

    # 3. QuÃ©t TÃªn biáº¿n chÃ­nh xÃ¡c (Exact Var Map)
    for keyword, var_name in exact_var_map.items():
        if keyword in text_lower:
            found_patterns.add(str(var_name).lower())

    # 4. Fallback thÃ´ng minh: Náº¿u khÃ´ng tÃ¬m tháº¥y keyword nÃ o tá»« map
    # Thá»­ láº¥y cÃ¡c tá»« láº¡ (khÃ´ng pháº£i location, khÃ´ng pháº£i tá»« phá»• thÃ´ng) lÃ m keyword
    if not found_patterns:
        words = text_lower.split()
        potential_keywords = []
        for w in words:
            # Náº¿u tá»« nÃ y khÃ´ng náº±m trong location_map vÃ  dÃ i > 2 kÃ½ tá»± -> cÃ³ thá»ƒ lÃ  tÃªn thiáº¿t bá»‹ (vÃ­ dá»¥: P19)
            is_loc = any(w in k for k in location_map.keys())
            if not is_loc and len(w) > 1 and w not in ["kiá»ƒm", "tra", "chart", "biá»ƒu", "Ä‘á»“"]:
                potential_keywords.append(w)
        
        # Chá»‰ thÃªm vÃ o náº¿u tÃ¬m tháº¥y Ã­t tá»« (trÃ¡nh add cáº£ cÃ¢u)
        if 0 < len(potential_keywords) < 3:
            found_patterns.update(potential_keywords)

    print(f"DEBUG: Keywords extracted -> {list(found_patterns)}")
    return list(found_patterns)

# Globals to store maps
location_map = {}
semantic_type_map = {}
exact_var_map = {}
dynamic_device_types = []
kg_records = []

def ensure_lookups(kg_df_input):
    global kg_df, _lookup_built, location_map, semantic_type_map, exact_var_map, dynamic_device_types, kg_records
    if _lookup_built and kg_df.equals(kg_df_input):
        return
    kg_df = kg_df_input
    location_map, semantic_type_map, exact_var_map, dynamic_device_types, kg_records = build_lookup_tables(kg_df)
    _lookup_built = True

# ================================================================
# ğŸ•“ INTERPRET RELATIVE TIME (giá»¯ nguyÃªn tá»« báº£n gá»‘c, Ä‘Ã£ tá»‘t)
# ================================================================
def interpret_relative_time(query: str):
    now = datetime.now()
    text = query.lower()

    if "hÃ´m nay" in text:
        start = now.replace(hour=0, minute=0, second=0)
        return start.strftime("%Y-%m-%d"), now.strftime("%Y-%m-%d")

    if "hÃ´m qua" in text:
        yesterday = now - timedelta(days=1)
        return yesterday.strftime("%Y-%m-%d"), yesterday.strftime("%Y-%m-%d")

    m = re.search(r"(\d+)\s*(ngÃ y|tuáº§n|thÃ¡ng|nÄƒm)", text)
    if m:
        n, unit = int(m.group(1)), m.group(2)
        if unit == "ngÃ y":   start = now - timedelta(days=n)
        elif unit == "tuáº§n": start = now - timedelta(weeks=n)
        elif unit == "thÃ¡ng":start = now - timedelta(days=n * 30)
        elif unit == "nÄƒm":  start = datetime(now.year - n + 1, 1, 1)
        return start.strftime("%Y-%m-%d"), now.strftime("%Y-%m-%d")

    if "tuáº§n nÃ y" in text:
        start = (now - timedelta(days=now.weekday())).replace(hour=0, minute=0, second=0)
        return start.strftime("%Y-%m-%d"), now.strftime("%Y-%m-%d")

    if "thÃ¡ng nÃ y" in text:
        return now.replace(day=1).strftime("%Y-%m-%d"), now.strftime("%Y-%m-%d")

    if "tá»« Ä‘áº§u nÄƒm" in text:
        return now.replace(month=1, day=1).strftime("%Y-%m-%d"), now.strftime("%Y-%m-%d")

    if "nÄƒm ngoÃ¡i" in text:
        y = now.year - 1
        return f"{y}-01-01", f"{y}-12-31"

    # NgÃ y tuyá»‡t Ä‘á»‘i dáº¡ng YYYY-MM-DD
    dates = re.findall(r"\d{4}-\d{2}-\d{2}", text)
    if len(dates) >= 2:
        return dates[0], dates[1]
    if len(dates) == 1:
        return dates[0], now.strftime("%Y-%m-%d")

    # NgÃ y dáº¡ng DD/MM/YYYY
    dates2 = re.findall(r"\d{1,2}/\d{1,2}/\d{4}", text)
    if dates2:
        parsed = [datetime.strptime(d, "%d/%m/%Y").strftime("%Y-%m-%d") for d in dates2]
        return (parsed[0], parsed[1]) if len(parsed) >= 2 else (parsed[0], now.strftime("%Y-%m-%d"))

    # Máº·c Ä‘á»‹nh: 30 ngÃ y gáº§n nháº¥t
    return (now - timedelta(days=30)).strftime("%Y-%m-%d"), now.strftime("%Y-%m-%d")

# ================================================================
# ğŸ” OFFLINE NLU ENGINE â€” thay tháº¿ hoÃ n toÃ n Gemini cho extraction
# ================================================================
def extract_location(text: str) -> str | None:
    """TÃ¬m vá»‹ trÃ­ trong cÃ¢u há»i báº±ng fuzzy match vá»›i location_map."""
    text_lower = text.lower()

    # 1. Khá»›p chÃ­nh xÃ¡c trÆ°á»›c
    for keyword, location in location_map.items():
        if keyword in text_lower:
            return location

    # 2. Fuzzy match náº¿u khÃ´ng khá»›p chÃ­nh xÃ¡c
    all_keys = list(location_map.keys())
    words = re.findall(r'\w+', text_lower)
    for word in words:
        matches = difflib.get_close_matches(word, all_keys, n=1, cutoff=0.82)
        if matches:
            return location_map[matches[0]]

    return None

def extract_data_types(text: str) -> list[str]:
    """TÃ¬m loáº¡i dá»¯ liá»‡u/thiáº¿t bá»‹ tá»« cÃ¢u há»i báº±ng Regex & Fuzzy Match."""
    text_lower = text.lower()
    found_patterns = set()

    # 1. Khá»›p báº±ng Word Boundary cho SEMANTIC_MAP (TrÃ¡nh lá»—i dÃ­nh chá»¯)
    for keyword, patterns in semantic_type_map.items():
        # DÃ¹ng regex \b Ä‘á»ƒ báº¯t chÃ­nh xÃ¡c tá»« khÃ³a Ä‘á»©ng Ä‘á»™c láº­p
        if re.search(r'\b' + re.escape(keyword) + r'\b', text_lower):
            found_patterns.update(patterns)

    # Náº¿u khÃ´ng pháº£i lÃ  regex keyword chuáº©n, kiá»ƒm tra dáº¡ng fuzzy trong tÃªn thiáº¿t bá»‹
    words = text_lower.split()
    for dt in dynamic_device_types:
        if dt in text_lower:
            found_patterns.add(dt)
        else:
            for word in words:
                if difflib.SequenceMatcher(None, word, dt).ratio() > 0.85:
                    found_patterns.add(dt)

    # 3. Khá»›p chÃ­nh xÃ¡c tÃªn biáº¿n (náº¿u ngÆ°á»i dÃ¹ng gÃµ tháº³ng tÃªn biáº¿n)
    for keyword, var_name in exact_var_map.items():
        if keyword in text_lower:
            found_patterns.add(var_name)
            
    # XÃ³a cÃ¡c tá»« stopword hoáº·c keywords dÆ° thá»«a ra, khÃ´ng tráº£ máº£ng empty
    if not found_patterns:
        # Nháº·t keyword tÃ¹y biáº¿n náº¿u query chá»©a cÃ¡c chuá»—i cÃ³ váº» nhÆ° tÃªn loáº¡i
        for word in words:
            if word not in ["kiá»ƒm", "tra", "táº¡i", "Ä‘á»‘t", "cÃ¡c", "thiáº¿t", "bá»‹", "ngÃ y", "thÃ¡ng", "hÃ´m", "nay", "qua"]:
                found_patterns.add(word)

    print(f"DEBUG: Keyword nháº­n diá»‡n Ä‘Æ°á»£c -> {list(found_patterns)}")
    return list(found_patterns)

def filter_devices_from_kg(location: str | None, data_type_patterns: list[str]) -> list[dict]:
    """
    NÃ¢ng cáº¥p: Sá»­ dá»¥ng cÆ¡ cháº¿ SCORING (TÃ­nh Ä‘iá»ƒm) Ä‘á»ƒ lá»c thiáº¿t bá»‹.
    Thay vÃ¬ match chÃ­nh xÃ¡c 100%, ta tÃ¬m thiáº¿t bá»‹ 'phÃ¹ há»£p nháº¥t'.
    """
    if not kg_records:
        return []

    # XÃ¡c Ä‘á»‹nh cÃ¡c cá»™t quan trá»ng trong KG
    cols = {
        "loc": next((c for c in kg_df.columns if "vá»‹ trÃ­" in c.lower() or "location" in c.lower()), ""),
        "name": next((c for c in kg_df.columns if "tÃªn thiáº¿t bá»‹" in c.lower()), "TÃªn thiáº¿t bá»‹"),
        "var":  next((c for c in kg_df.columns if "tÃªn biáº¿n" in c.lower() or "key" in c.lower()), "TÃªn biáº¿n"),
        "type": next((c for c in kg_df.columns if "loáº¡i" in c.lower()), "Loáº¡i thiáº¿t bá»‹"),
        "dev":  next((c for c in kg_df.columns if c.lower() == "device"), "Device")
    }

    candidates = []

    # Chuáº©n hÃ³a patterns
    patterns = [str(p).lower().strip() for p in data_type_patterns if str(p).strip()]

    for row in kg_records:
        # Láº¥y giÃ¡ trá»‹ cÃ¡c trÆ°á»ng
        r_loc  = str(row.get(cols["loc"], "")).lower()
        r_name = str(row.get(cols["name"], "")).lower()
        r_var  = str(row.get(cols["var"], "")).lower()
        r_type = str(row.get(cols["type"], "")).lower()
        r_dev  = str(row.get(cols["dev"], "")).lower()

        # 1. Lá»c Vá»‹ trÃ­ (Location Filter) - Báº®T BUá»˜C náº¿u user cÃ³ nháº­p location
        if location:
            # Logic fuzzy nháº¹ cho location: "sk5" match "Ä‘á»‘t sk5"
            if location.lower() not in r_loc and location.lower() not in r_name:
                continue 

        # 2. TÃ­nh Ä‘iá»ƒm khá»›p keyword (Scoring)
        score = 0
        if not patterns:
            # Náº¿u user khÃ´ng nháº­p loáº¡i biáº¿n (vd: "Kiá»ƒm tra SK5") -> Láº¥y háº¿t -> Score máº·c Ä‘á»‹nh = 1
            score = 1
        else:
            for p in patterns:
                # Æ¯u tiÃªn khá»›p trong TÃªn biáº¿n (Key) hoáº·c Loáº¡i thiáº¿t bá»‹
                if p in r_var:  score += 3  # Match 'contrainte' trong 'Contrainte(13)'
                if p in r_type: score += 3  # Match 'sg' trong loáº¡i 'sg'
                if p in r_name: score += 1  # Match trong tÃªn hiá»ƒn thá»‹
                if p in r_dev:  score += 1  # Match trong tÃªn Device cha

        if score > 0:
            candidates.append({
                "data": row,
                "score": score
            })

    # 3. Sáº¯p xáº¿p vÃ  chá»n lá»c
    if not candidates:
        return []

    # Sáº¯p xáº¿p giáº£m dáº§n theo Ä‘iá»ƒm
    candidates.sort(key=lambda x: x["score"], reverse=True)

    # Náº¿u cÃ³ patterns, chá»‰ láº¥y nhá»¯ng tháº±ng cÃ³ score cao nháº¥t (trÃ¡nh láº¥y rÃ¡c)
    # VÃ­ dá»¥: Náº¿u top 1 lÃ  3 Ä‘iá»ƒm, chá»‰ láº¥y nhá»¯ng tháº±ng >= 2 Ä‘iá»ƒm
    if patterns:
        best_score = candidates[0]["score"]
        threshold = max(1, best_score - 1) # Cho phÃ©p sai sá»‘ nháº¹
        final_list = [c["data"] for c in candidates if c["score"] >= threshold]
    else:
        # Náº¿u khÃ´ng cÃ³ pattern (láº¥y háº¿t theo location), tráº£ vá» táº¥t cáº£ káº¿t quáº£ tÃ¬m Ä‘Æ°á»£c
        final_list = [c["data"] for c in candidates]

    # Format láº¡i output chuáº©n
    results = []
    for item in final_list:
        results.append({
            "TÃªn thiáº¿t bá»‹": item.get(cols["name"], ""),
            "Device": item.get(cols["dev"], ""),
            "TÃªn biáº¿n": item.get(cols["var"], ""),
            "Loáº¡i thiáº¿t bá»‹": item.get(cols["type"], "")
        })

    return results

def parse_query_offline(query: str) -> dict:
    """
    ğŸ”‘ HÃ€M CHÃNH â€” thay tháº¿ hoÃ n toÃ n chatbot() + Gemini.
    PhÃ¢n tÃ­ch cÃ¢u há»i 100% offline, khÃ´ng gá»i API.
    """
    text = query.strip()

    # 0. Intent 
    intent = "chart"
    check_keywords = ["kiá»ƒm tra", "check", "tráº¡ng thÃ¡i", "status", "hoáº¡t Ä‘á»™ng khÃ´ng", "bÃ¡o cÃ¡o"]
    if any(k in text.lower() for k in check_keywords):
        intent = "check_status"

    # 1. Thá»i gian
    start_date, end_date = interpret_relative_time(text)

    # 2. Vá»‹ trÃ­
    location = extract_location(text)

    # 3. Loáº¡i dá»¯ liá»‡u / Thiáº¿t bá»‹
    data_type_patterns = extract_data_types(text)
    print(f"DEBUG: parse_query_offline - data_type_patterns from extract_data_types: {data_type_patterns}")

    # 4. Lá»c devices tá»« KG
    devices = filter_devices_from_kg(location, data_type_patterns)
    print(f"DEBUG: parse_query_offline - devices after first filter (with patterns): {devices}")

    # 5. Fallback: Náº¿u khÃ´ng tÃ¬m tháº¥y loáº¡i dá»¯ liá»‡u nhÆ°ng cÃ³ vá»‹ trÃ­ â†’ láº¥y táº¥t cáº£
    if not devices and location:
        print(f"   â„¹ï¸  KhÃ´ng rÃµ loáº¡i thiáº¿t bá»‹ â€” láº¥y táº¥t cáº£ biáº¿n táº¡i '{location}'")
        devices = filter_devices_from_kg(location, [])
        print(f"DEBUG: parse_query_offline - devices after fallback filter (no patterns): {devices}")

    # ÄÃ³ng ngoáº·c result (Ä‘oáº¡n code cá»§a báº¡n bá»‹ thiáº¿u pháº§n nÃ y)
    result = {
        "intent": intent,
        "start_date": start_date,
        "end_date": end_date,
        "location": location,
        "data_type_patterns": data_type_patterns,
        "devices_found": len(devices),
        "devices": devices
    }

    return result

# ================================================================
# ğŸ“Š OFFLINE ANALYSIS ENGINE â€” thay tháº¿ Gemini analyze_data()
# ================================================================

# NgÆ°á»¡ng cáº£nh bÃ¡o theo loáº¡i biáº¿n (tÃ¹y chá»‰nh theo dá»± Ã¡n cá»§a báº¡n)
THRESHOLDS = {
    "temperature": {"warn_high": 45, "warn_low": -5,  "unit": "Â°C"},
    "temp":        {"warn_high": 45, "warn_low": -5,  "unit": "Â°C"},
    "humidity":    {"warn_high": 95, "warn_low": 20,  "unit": "%"},
    "strain":      {"warn_high": 500, "warn_low": -500,"unit": "Î¼Îµ"},
    "sg":          {"warn_high": 500, "warn_low": -500,"unit": "Î¼Îµ"},
    "acc":         {"warn_high": 0.5, "warn_low": -0.5,"unit": "g"},
    "vibration":   {"warn_high": 0.5, "warn_low": -0.5,"unit": "g"},
    "rainfall":    {"warn_high": 100, "warn_low": 0,  "unit": "mm"},
    "wind":        {"warn_high": 30,  "warn_low": 0,  "unit": "m/s"},
    "water_level": {"warn_high": 10,  "warn_low": -10, "unit": "m"},
}

def detect_anomalies(df: pd.DataFrame, key_lower: str) -> list[str]:
    """PhÃ¡t hiá»‡n báº¥t thÆ°á»ng dá»±a trÃªn IQR + threshold."""
    alerts = []
    if df.empty or "value" not in df.columns:
        return alerts

    # IQR method
    Q1, Q3 = df["value"].quantile(0.25), df["value"].quantile(0.75)
    IQR = Q3 - Q1
    outliers = df[(df["value"] < Q1 - 2.5 * IQR) | (df["value"] > Q3 + 2.5 * IQR)]
    if len(outliers) > 0:
        alerts.append(f"âš ï¸  PhÃ¡t hiá»‡n {len(outliers)} Ä‘iá»ƒm báº¥t thÆ°á»ng (ngoÃ i IQRÃ—2.5)")

    # Threshold check
    for key_pattern, limits in THRESHOLDS.items():
        if key_pattern in key_lower:
            hi, lo = limits["warn_high"], limits["warn_low"]
            over_high = (df["value"] > hi).sum()
            over_low  = (df["value"] < lo).sum()
            if over_high > 0:
                alerts.append(f"ğŸ”´ {over_high} Ä‘iá»ƒm vÆ°á»£t ngÆ°á»¡ng cao ({hi} {limits['unit']})")
            if over_low > 0:
                alerts.append(f"ğŸ”µ {over_low} Ä‘iá»ƒm dÆ°á»›i ngÆ°á»¡ng tháº¥p ({lo} {limits['unit']})")
            break

    return alerts

def trend_description(df: pd.DataFrame) -> str:
    """MÃ´ táº£ xu hÆ°á»›ng tÄƒng/giáº£m/á»•n Ä‘á»‹nh báº±ng linear regression Ä‘Æ¡n giáº£n."""
    if len(df) < 3:
        return "khÃ´ng Ä‘á»§ dá»¯ liá»‡u Ä‘á»ƒ phÃ¢n tÃ­ch xu hÆ°á»›ng"

    x = np.arange(len(df))
    y = df["value"].values
    slope = np.polyfit(x, y, 1)[0]

    std = df["value"].std()
    mean = df["value"].mean()
    cv = (std / mean * 100) if mean != 0 else 0  # Coefficient of variation

    if abs(slope) < 0.001 * abs(mean):
        trend = "á»•n Ä‘á»‹nh"
    elif slope > 0:
        trend = f"cÃ³ xu hÆ°á»›ng TÄ‚NG nháº¹ (+{slope:.4f}/Ä‘iá»ƒm)"
    else:
        trend = f"cÃ³ xu hÆ°á»›ng GIáº¢M nháº¹ ({slope:.4f}/Ä‘iá»ƒm)"

    stability = "biáº¿n Ä‘á»™ng cao" if cv > 30 else ("biáº¿n Ä‘á»™ng vá»«a" if cv > 10 else "á»•n Ä‘á»‹nh")
    return f"{trend}, {stability} (CV={cv:.1f}%)"

def analyze_data_offline(fetched_data_list: list, original_query: str):
    """
    ğŸ”‘ PhÃ¢n tÃ­ch thá»‘ng kÃª + phÃ¡t hiá»‡n báº¥t thÆ°á»ng + mÃ´ táº£ xu hÆ°á»›ng + Láº¥y giÃ¡ trá»‹ má»›i nháº¥t.
    """
    print("\n" + "=" * 50)
    print("ğŸ” PHÃ‚N TÃCH Dá»® LIá»†U (Offline Engine)")
    print("=" * 50)

    if not fetched_data_list:
        print("   - KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ phÃ¢n tÃ­ch.")
        return

    for item in fetched_data_list:
        df   = item["data"]
        label = item["label"]
        key_lower = label.lower()

        print(f"\nğŸ“Œ {label}")
        print("   " + "-" * 46)

        if df.empty or "value" not in df.columns:
            print("   â„¹ï¸  KhÃ´ng cÃ³ dá»¯ liá»‡u.")
            continue

        mean_val = df["value"].mean()
        min_val  = df["value"].min()
        max_val  = df["value"].max()
        std_val  = df["value"].std()
        n        = len(df)

        # --- Láº¤Y GIÃ TRá»Š Má»šI NHáº¤T á» ÄÃ‚Y ---
        latest_row = df.iloc[-1]
        latest_val = latest_row["value"]
        latest_ts = latest_row["ts"].strftime('%Y-%m-%d %H:%M:%S')

        # TÃ¬m unit tá»« THRESHOLDS
        unit = ""
        for k, v in THRESHOLDS.items():
            if k in key_lower:
                unit = v["unit"]
                break

        # Thá»‘ng kÃª cÆ¡ báº£n
        print(f"   ğŸŸ¢ Má»šI NHáº¤T       : {latest_val:.3f} {unit} (Cáº­p nháº­t lÃºc {latest_ts})")
        print(f"   ğŸ“Š Sá»‘ Ä‘iá»ƒm dá»¯ liá»‡u: {n}")
        print(f"   ğŸ“ˆ Cao nháº¥t       : {max_val:.3f} {unit}")
        print(f"   ğŸ“‰ Tháº¥p nháº¥t      : {min_val:.3f} {unit}")
        print(f"   ã€°ï¸  Trung bÃ¬nh     : {mean_val:.3f} {unit}")

        # Thá»i gian Ä‘á»‰nh
        idx_max = df["value"].idxmax()
        idx_min = df["value"].idxmin()
        print(f"   ğŸ• Äá»‰nh cao nháº¥t lÃºc: {df.loc[idx_max, 'ts'].strftime('%Y-%m-%d %H:%M')}")
        print(f"   ğŸ• Äá»‰nh tháº¥p nháº¥t lÃºc: {df.loc[idx_min, 'ts'].strftime('%Y-%m-%d %H:%M')}")

        # Xu hÆ°á»›ng
        trend = trend_description(df)
        print(f"   ğŸ“¡ Xu hÆ°á»›ng       : {trend}")

        # Cáº£nh bÃ¡o báº¥t thÆ°á»ng
        alerts = detect_anomalies(df, key_lower)
        if alerts:
            print("   ğŸš¨ Cáº£nh bÃ¡o:")
            for a in alerts:
                print(f"      {a}")
        else:
            print("   âœ… KhÃ´ng phÃ¡t hiá»‡n báº¥t thÆ°á»ng")

    print("\n" + "=" * 50)

# ================================================================
# ğŸ›‘ NEWSENSE CLIENT (giá»¯ nguyÃªn tá»« báº£n gá»‘c)
# ================================================================
class NewsenseClient:
    def __init__(self, base_url, username, password):
        self.base_url = base_url.rstrip('/')
        self.username = username
        self.password = password
        self.token = self.login()

    def login(self):
        resp = requests.post(f"{self.base_url}/auth/login",
                             json={"username": self.username, "password": self.password})
        if resp.status_code == 401:
            raise Exception("âŒ Sai username hoáº·c máº­t kháº©u.")
        resp.raise_for_status()
        print("âœ… ÄÄƒng nháº­p Newsense thÃ nh cÃ´ng.")
        return resp.json().get("token")

    def get_devices(self):
        headers = {"X-Authorization": f"Bearer {self.token}"}
        page, devices = 0, []
        while True:
            resp = requests.get(f"{self.base_url}/tenant/devices", headers=headers,
                                params={"pageSize": 100, "page": page})
            resp.raise_for_status()
            data = resp.json()
            for d in data.get("data", []):
                devices.append({"id": d["id"]["id"], "name": d["name"]})
            if not data.get("hasNextPage"):
                break
            page += 1
        return devices

    def get_timeseries(self, device_id, key, start_date_str, end_date_str):
        headers = {"X-Authorization": f"Bearer {self.token}"}
        url = f"{self.base_url}/plugins/telemetry/DEVICE/{device_id}/values/timeseries"

        try:
            start_dt = datetime.strptime(start_date_str, "%Y-%m-%d").replace(hour=0, minute=0, second=0)
            end_dt   = datetime.strptime(end_date_str,   "%Y-%m-%d").replace(hour=23, minute=59, second=59)
            start_ts = int(start_dt.timestamp() * 1000)
            end_ts   = int(end_dt.timestamp()   * 1000)
        except ValueError:
            print("âŒ Äá»‹nh dáº¡ng ngÃ y khÃ´ng há»£p lá»‡.")
            return pd.DataFrame()

        duration_days = (end_dt - start_dt).days
        params = {"startTs": start_ts, "endTs": end_ts, "keys": key}

        if duration_days > 90:
            params.update({"interval": 86400000 * 7, "agg": "AVG"})
            print(f"   - Tá»•ng há»£p theo TUáº¦N cho {key}")
        elif duration_days > 7:
            params.update({"interval": 86400000, "agg": "AVG"})
            print(f"   - Tá»•ng há»£p theo NGÃ€Y cho {key}")
        else:
            params["limit"] = 10000
            print(f"   - Dá»¯ liá»‡u thÃ´ cho {key}")

        resp = requests.get(url, headers=headers, params=params)
        if resp.status_code != 200:
            print(f"âš ï¸ HTTP {resp.status_code}: {resp.text[:200]}")
            return pd.DataFrame()

        data = resp.json().get(key, [])
        if not data:
            return pd.DataFrame()

        df = pd.DataFrame(data)
        try:
            df["ts"] = pd.to_datetime(df["timestamp"], unit="ms")
        except:
            df["ts"] = pd.to_datetime(df["ts"], unit ="ms")
        df["value"] = pd.to_numeric(df["value"], errors="coerce")
        return df[["ts", "value"]].dropna(subset=["value"])

    def get_latest_telemetry(self, device_id, device_name, keys, time_delay_seconds=3600):
        if not keys: return []

        headers = {"X-Authorization": f"Bearer {self.token}"}
        keys_str = ",".join(keys)
        url = f"{self.base_url}/plugins/telemetry/DEVICE/{device_id}/values/timeseries?keys={keys_str}&limit=1"

        results = []
        try:
            resp = requests.get(url, headers=headers)
            data = resp.json()
            now = datetime.now(timezone.utc)

            for key in keys:
                if key in data and len(data[key]) > 0:
                    last_ts = data[key][0]['ts']
                    last_time = datetime.fromtimestamp(last_ts / 1000, tz=timezone.utc)
                    diff = now - last_time

                    status = "ğŸŸ¢ Active" if diff.total_seconds() < time_delay_seconds else "ğŸ”´ Stopped"

                    results.append({
                        "Device Name": device_name,
                        "Variable (Key)": key,
                        "Last Update": last_time.astimezone().strftime('%Y-%m-%d %H:%M:%S'),
                        "Delay": f"{diff.days}d {diff.seconds//3600}h {(diff.seconds%3600)//60}m",
                        "Status": status
                    })
                else:
                    results.append({
                        "Device Name": device_name,
                        "Variable (Key)": key,
                        "Last Update": "Never",
                        "Delay": "N/A",
                        "Status": "âšª No Data"
                    })
        except Exception as e:
            print(f"âš ï¸ Lá»—i khi quÃ©t {device_name}: {e}")
        return results


# ================================================================
# ğŸ”§ INTERFACE FOR STREAMLIT (app.py)
# ================================================================
def chatbot(query: str, kg_df_input: pd.DataFrame, chat_history: list):
    ensure_lookups(kg_df_input)
    parsed = parse_query_offline(query)
    is_latest = ("hiá»‡n táº¡i" in query.lower() or "má»›i nháº¥t" in query.lower())
    result = {
        "intent": parsed.get("intent", "chart"),
        "start_date": parsed.get("start_date"),
        "end_date": parsed.get("end_date"),
        "is_latest": is_latest,
        "devices": parsed.get("devices", [])
    }
    result_str = json.dumps(result, ensure_ascii=False)
    chat_history.append({"role": "model", "content": result_str})
    return result_str, chat_history

def analyze_data(fetched_data_list: list, original_query: str):
    output = []
    output.append("=" * 50)
    output.append("ğŸ” PHÃ‚N TÃCH Dá»® LIá»†U (Offline Engine)")
    output.append("=" * 50)
    if not fetched_data_list:
        return "KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ phÃ¢n tÃ­ch."
    for item in fetched_data_list:
        df   = item["data"]
        label = item["label"]
        key_lower = label.lower()
        output.append(f"\nğŸ“Œ {label}")
        output.append("-" * 46)
        if df.empty or "value" not in df.columns:
            output.append("â„¹ï¸  KhÃ´ng cÃ³ dá»¯ liá»‡u.")
            continue
        mean_val = df["value"].mean()
        min_val  = df["value"].min()
        max_val  = df["value"].max()
        n        = len(df)
        latest_row = df.iloc[-1]
        latest_val = latest_row["value"]
        latest_ts = latest_row["ts"].strftime('%Y-%m-%d %H:%M:%S')
        unit = ""
        for k, v in THRESHOLDS.items():
            if k in key_lower:
                unit = v["unit"]
                break
        output.append(f"ğŸŸ¢ Má»šI NHáº¤T       : {latest_val:.3f} {unit} (Cáº­p nháº­t lÃºc {latest_ts})")
        output.append(f"ğŸ“Š Sá»‘ Ä‘iá»ƒm dá»¯ liá»‡u: {n}")
        output.append(f"ğŸ“ˆ Cao nháº¥t       : {max_val:.3f} {unit}")
        output.append(f"ğŸ“‰ Tháº¥p nháº¥t      : {min_val:.3f} {unit}")
        output.append(f"ã€°ï¸  Trung bÃ¬nh     : {mean_val:.3f} {unit}")
        idx_max = df["value"].idxmax()
        idx_min = df["value"].idxmin()
        output.append(f"ğŸ• Äá»‰nh cao nháº¥t lÃºc: {df.loc[idx_max, 'ts'].strftime('%Y-%m-%d %H:%M')}")
        output.append(f"ğŸ• Äá»‰nh tháº¥p nháº¥t lÃºc: {df.loc[idx_min, 'ts'].strftime('%Y-%m-%d %H:%M')}")
        trend = trend_description(df)
        output.append(f"ğŸ“¡ Xu hÆ°á»›ng       : {trend}")
        alerts = detect_anomalies(df, key_lower)
        if alerts:
            output.append("ğŸš¨ Cáº£nh bÃ¡o:")
            for a in alerts:
                output.append(f"      {a}")
        else:
            output.append("âœ… KhÃ´ng phÃ¡t hiá»‡n báº¥t thÆ°á»ng")
    output.append("\n" + "=" * 50)
    return "\n".join(output)

if __name__ == "__main__":
    # ================================================================
    # ğŸš€ MAIN LOOP
    # ================================================================
    try:
        client = NewsenseClient(BASE_URL, TB_USER, TB_PASS)
        all_devices = client.get_devices()
        device_name_to_id_map = {d["name"]: d["id"] for d in all_devices}
        print(f"âœ… ÄÃ£ táº£i {len(device_name_to_id_map)} thiáº¿t bá»‹ tá»« Newsense.")
    except Exception as e:
        print(f"âŒ KhÃ´ng thá»ƒ khá»Ÿi táº¡o Newsense Client: {e}")
        client = None
        device_name_to_id_map = {}

    print("\nğŸ¤– Chatbot Offline sáºµn sÃ ng. GÃµ 'exit' Ä‘á»ƒ thoÃ¡t.")
    print("ğŸ’¡ VÃ­ dá»¥: 'GNSS 3 ngÃ y gáº§n Ä‘Ã¢y'\n")

    while True:
        # 1. Báº£o vá»‡ toÃ n bá»™ quÃ¡ trÃ¬nh báº±ng try...except Ä‘á»ƒ khÃ´ng bá»‹ sáº­p vÃ²ng láº·p
        try:
            query = input("\nBáº¡n: ").strip()
            if not query:
                continue

            if query.lower() in ["exit", "quit", "q", "thoÃ¡t"]:
                print("ğŸ‘‹ Táº¡m biá»‡t!")
                break

            if not client:
                print("âŒ Client Newsense chÆ°a khá»Ÿi táº¡o.")
                continue

            # â”€â”€ BÆ¯á»šC 1: PhÃ¢n tÃ­ch offline (khÃ´ng cáº§n Gemini) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            result = parse_query_offline(query)

            print("\nğŸ§  Káº¿t quáº£ phÃ¢n tÃ­ch (Offline):")
            print(json.dumps(result, ensure_ascii=False, indent=2))

            devices_to_plot = result.get("devices", [])
            start_date      = result.get("start_date")
            end_date        = result.get("end_date")

            if not devices_to_plot:
                print("â„¹ï¸  KhÃ´ng tÃ¬m tháº¥y thiáº¿t bá»‹ phÃ¹ há»£p.")
                print("   Gá»£i Ã½: ThÃªm tÃªn vá»‹ trÃ­ cá»¥ thá»ƒ vÃ o cÃ¢u há»i.")
                continue

            if not start_date or not end_date:
                print("âš ï¸  KhÃ´ng xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c khoáº£ng thá»i gian.")
                continue

            # â”€â”€ BÆ¯á»šC 2: Láº¥y dá»¯ liá»‡u tá»« Newsense â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            print("\nâ³ Äang táº£i dá»¯ liá»‡u tá»« Newsense...")
            fetched_data = []

            for device_info in devices_to_plot:
                device_name     = device_info.get("Device")
                data_key        = device_info.get("TÃªn biáº¿n")
                full_name_label = device_info.get("TÃªn thiáº¿t bá»‹", device_name)

                if not device_name or not data_key:
                    continue

                device_id = device_name_to_id_map.get(device_name)
                if not device_id:
                    matches = difflib.get_close_matches(device_name, device_name_to_id_map.keys(), n=1, cutoff=0.8)
                    if matches:
                        print(f"   - Fuzzy match: '{device_name}' â†’ '{matches[0]}'")
                        device_id = device_name_to_id_map[matches[0]]
                    else:
                        print(f"   - âš ï¸ KhÃ´ng tÃ¬m tháº¥y: '{device_name}'")
                        continue

                print(f"   - Äang láº¥y: {full_name_label} ({data_key})")

                # Báº¯t lá»—i riÃªng khi táº£i API Ä‘á»ƒ trÃ¡nh sáº­p má»™t thiáº¿t bá»‹ lÃ m há»ng cáº£ quÃ¡ trÃ¬nh
                try:
                    df = client.get_timeseries(device_id, data_key, start_date, end_date)
                    if not df.empty:
                        fetched_data.append({
                            "label": f"{full_name_label} ({data_key})",
                            "data": df
                        })
                    else:
                        print(f"   - â„¹ï¸  KhÃ´ng cÃ³ dá»¯ liá»‡u cho {full_name_label}")
                except Exception as e:
                    print(f"   - âŒ Lá»—i khi táº£i {device_name}: {e}")

            if not fetched_data:
                print("ğŸš« KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ hiá»ƒn thá»‹ biá»ƒu Ä‘á»“/phÃ¢n tÃ­ch.")
                continue

            # â”€â”€ BÆ¯á»šC 3: Váº½ biá»ƒu Ä‘á»“ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            print("\nğŸ“ˆ Äang táº¡o biá»ƒu Ä‘á»“...")
            num_plots = len(fetched_data)
            fig, axes = plt.subplots(num_plots, 1, figsize=(15, 5 * num_plots), squeeze=False)
            fig.suptitle(f"{query}", fontsize=16, y=1.01)

            for i, item in enumerate(fetched_data):
                df, label = item["data"], item["label"]
                ax = axes[i][0]
                ax.plot(df["ts"], df["value"], label=label, marker=".", linestyle="-", linewidth=1)
                ax.set_title(label, fontsize=12)
                ax.set_xlabel("Thá»i gian")
                ax.set_ylabel("GiÃ¡ trá»‹")
                ax.legend(loc="best")
                ax.grid(True, linestyle="--", linewidth=0.5)

            plt.tight_layout()
            plt.show()

            # FIX QUAN TRá»ŒNG: XÃ³a biá»ƒu Ä‘á»“ khá»i bá»™ nhá»› Colab Ä‘á»ƒ trÃ¡nh treo cho láº§n nháº­p sau
            plt.close('all')

            # â”€â”€ BÆ¯á»šC 4: PhÃ¢n tÃ­ch offline (khÃ´ng cáº§n Gemini) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            analyze_data_offline(fetched_data, query)

            print("\n" + "âœ¨" * 25)
            print("âœ… ÄÃ£ xá»­ lÃ½ xong! Má»i báº¡n cuá»™n xuá»‘ng vÃ  nháº­p cÃ¢u há»i tiáº¿p theo.")
            print("âœ¨" * 25)

        except Exception as e:
            # Náº¿u cÃ³ lá»—i ngáº§m xáº£y ra á»Ÿ báº¥t ká»³ Ä‘Ã¢u, in ra lá»—i vÃ  tiáº¿p tá»¥c vÃ²ng láº·p
            print(f"\nâŒ Lá»–I Há»† THá»NG: {e}")
            import traceback
            traceback.print_exc()
            print("ğŸ”„ Chatbot Ä‘Ã£ tá»± Ä‘á»™ng phá»¥c há»“i. Báº¡n cÃ³ thá»ƒ thá»­ cÃ¢u há»i khÃ¡c.")

